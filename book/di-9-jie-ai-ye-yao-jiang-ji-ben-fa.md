# 第9节 AI也要讲基本法

虽然模型已经可以对话，能够答复用户的问题，但在实际体验中很可能并不好用。

回想与人聊天的情况，和一些人对话能感到如沐春风，和另一些人则话不投机半句多。表达同样的意思，有的人可以说的很让人信服，而有的人则只会让人感到冒犯。这是聊天的艺术，不光人类，AI也需要考虑这个问题。

作为人类，我们其实并不需要高超的聊天技能，因为每个人从事不同的职业，社交不一定是必需品。但作为一个面向全世界用户的聊天AI，它需要考虑的就多了。它必须具有中立的普世价值观，不能歧视少数群体，不能表达极端观点。要遵守各个国家的法律，尊重不同民族的习俗。说话态度要积极向上，多传递正能量，弘扬社会正义。面对用户不合理的请求，一定要拒绝，但也不能激怒对方。总之，面对众口难调的用户，AI必须谨小慎微，不能说错话。

如果ChatGPT是一个未与人类偏好对齐的AI，别有用心的人就可能引诱AI透露危险的事情。比如，人们可以问AI“如何在家制作炸药？”这不是个玩笑。AI上知天文下知地理，制作炸药的步骤它可以信手拈来。以前，一个反社会分子想要知道怎么制作炸药，必须得学几年化学。然而现在，AI可以手把手教他。看起来人畜无害的AI此刻就成了恐怖分子的帮凶。类似的场景数不胜数，聪明人既可以做出科研突破，也有可能摇身一变成为邪恶的源泉。因此，与人类偏好对齐非常重要。

那么该如何做呢？回想前面的预训练和指令微调，模型其实都是在学习现成的句子。模型看到什么文字，就会模仿那样说话。在海量的训练数据中，一定会掺杂着不安全、不道德的内容。一个简单直接的办法是从训练数据中去掉它们，眼不见为净。如果模型从来没看过关于制造炸药的知识，它自然也就没法告诉用户。

但这种直觉上正确的做法其实并不好用。一方面，就算模型没读过如何制造炸药，它也完全可以通过化学知识和关于炸药的常识自己推断出来。另一方面，很多语料是没法清晰定义安全性的。一个生物化学方面的论文，既是社会发展的推动力，也可能蕴含着危险的因素，我们不可能因为研究有可能产生危害就不做这个研究。很多事情都是双刃剑，如果拒绝一个语料可能带来的坏处，就同时隔绝了它所能带来的好处。此外，与人类偏好对齐并不仅仅需要考虑安全性问题，价值观、道德、礼貌等方面也同样重要。难道要把不礼貌的语料从训练数据中全部去掉？那样的话训练数据估计就没多少了。

所以，我们不可能在现有的训练模式中解决这个问题，必须用一种新的方法。

如果仔细思考训练数据的模式，可以发现，我们一直在教模型应该怎样做，却从来没告诉过它不该怎样做。而与人类偏好对齐最合理的方式应该是同时告诉它不要怎样做。也就是说，我们应该制定一些规则，然后生成一大批新的训练样本。在这些样本中，一半作为正样本，告诉模型你可以学着这样说话。另一半作为负样本，告诉模型你不要说这样的话。模型在训练过程中同时观看正负样本，从而能够明辨是非，形成自己的道德观和价值观。

既然如此，让我们想想如何具体实施。在[2.8节](broken-reference)的图2.8中，训练的目标是让最后实际输出的概率分布与期望ID一致，训练过程中会不断优化使得误差越来越小。想象如果我们现在有两个训练样本，一正一负。正样本其实和以往的训练数据用法一样，只要优化它与期望输出之间的差距就好。但负样本呢？如果把负样本放进去优化，模型就会越学越差。所以我们应该反过来，让模型学习增大负样本实际输出与期望输出之间的差距。这个差距越大，说明模型越来越不会像负样本那样说话。

于是，我们可以重新设计优化目标。把原本的交叉熵损失作为正损失，然后额外增加一个负样本的交叉熵损失，作为负损失。我们不是把两个损失加起来，而是用正损失减去负损失，作为最终的损失。此处的减法相当于优化负的负样本损失。我们可以这样理解，由于优化交叉熵损失就是减小实际输出和期望输出之间的差距，那么优化负的交叉熵损失显然就是增大差距了。下式概括了优化目标的形式，通过调整模型参数让正样本损失越来越接近0、负样本损失越来越大，两者之差越来越小。其中，θ代表模型的参数，p代表正样本，n代表负样本。

$$
\min_{\theta} \left( \text{loss} \left( \theta, p \right) - \text{loss} \left( \theta, n \right) \right)
$$

优化新的损失可以让模型既接近正样本又远离负样本，一举两得。可能有读者会感到疑惑，为什么不单独优化负样本呢，只保留带负号的第二项不就行了。没错，如果单独优化负样本，模型的确会远离负样本，但这种情况下很容易导致模型本身的语言能力退化，以至于连话都不会说了。道理很简单，我们的负样本损失只要求模型的输出远离预期，然而这个要求简直太容易达到了，胡言乱语甚至输出随机字符都可以满足这个要求。但我们想要的其实是在正常说话的同时远离负样本，这就意味着必须有一个正样本时刻牵扯着负样本，强迫模型优化正负样本之间的差距。

这种训练方式看起来不难，但实际操作上有一些麻烦。最大的问题是正负样本损失不具有可比性。比如，我们给2.9节的例子加一个负样本。负样本回答完全缺乏咨询师应有的同理心，冷漠、粗鲁、随意地应付来访者。

> 用户问题：我最近很郁闷，能告诉我该怎么办吗
>
> 正样本回答：我理解你现在的感受，感觉郁闷常常会让人有些迷茫。你能告诉我是什么让你感到郁闷吗？
>
> 负样本回答：你先说说你为什么郁闷吧。

按照设定，我们希望训练模型，让正样本的损失变小，同时负样本损失变大。然而，如果我们在训练之前就计算正负样本的损失，会发现它们本身就有一定差距。而这个差距是否意味着模型本身就对正负样本有所偏好呢？

其实不是。要知道，用于训练的数据集包含了成千上万的样本，里面任何一句话，单独拿出来计算误差，结果都是不同的。也就是说，我们其实是在努力让整个数据集总体误差更小，而不是盯着某条数据优化。这样的话，单独的一条正样本和负样本误差不同很正常，而且优化的目标也不是缩小或增大这个差距。我们需要一个更明确，且与特定数据无关的衡量标准。

回顾最初的目的，我们希望对齐后的模型相对于原始模型更能区分正样本和负样本。于是，这里其实有一个天然的标准，就是原模型。虽然正负样本的误差不可比较，但正样本在原模型和新模型上的误差则可以比较，而且非常契合我们的需求。这就好像自我成长，模型和过去的自己比较，逐渐成长为期待的样子。

于是，我们把模型复制一份。一个作为原模型，也叫做参考模型，另一个是新模型，将成为对齐后的模型。然后，我们锁定参考模型的参数，让它保持不变。构造下式所示的损失函数。

$$
\min_{\theta} \left( \text{loss} (\left( \theta, p \right) - \text{loss} \left( \theta_0, p \right) ) + (\text{loss} \left( \theta_0, n \right) - \text{loss} \left( \theta, n \right)) \right)
$$

在这个新的损失函数中，优化目标变成了两项。第一项是新旧模型在正样本上损失之差，这个差越接近负无穷，意味着新模型的误差越小。第二项是旧新模型在负样本上损失之差，这个差越接近负无穷，意味着新模型的误差越大。按照这个目标优化，新模型就会越擅长预测正样本，同时更不善于输出负样本。

现在，回到我们的主线任务，尝试用这种方法调教我们的写诗模型。

首先，我们要确定想要对齐的偏好是什么。在写诗的过程中，诗人可以有很多个人考量的角度，比如格律（七言还是五言，律诗还是绝句）、流派（豪放还是婉约）、内容（叙事还是抒情）等等。让模型偏好某种风格，正如古代的诗人一样。这种偏好不代表水平高低，正如将模型与人类偏好对齐也并非模型能力上的进步。出于教学的目的，我选择让模型偏好五言诗。这种风格的改变最容易被读者观察到，而且也方便我们构建训练数据。

接下来，我们把训练数据中的五言诗和非五言诗分成两组。将其随机配对，形成正负样本的集合。每一对正负样本大概长这样。

{% code overflow="wrap" %}
```
('<INS>請用以下題目寫一首詩<INP>宿壽安甘棠館 二<RES>山空蕙氣香，乳管折雲房。\n願值壺中客，親傳肘後方。\n三更禮星斗，寸匕服丹霜。\n默坐樹陰下，仙經橫石床。', '<INS>請用以下題目寫一首詩<INP>遣懷<RES>落魄江南載酒行，楚腰腸斷掌中輕。\n十年一覺揚州夢，贏得青樓薄倖名。')
```
{% endcode %}

可以看到，无论正样本还是负样本，它们的格式与指令微调时并无两样，因为我们并不想改变模型与人们交互的方式。正样本是随机取的一首五言诗，来自唐代诗人尚颜的《宿寿安甘棠 二》。负样本则是随机取的一首七言诗，正是著名诗人杜牧的《遣怀》，想必许多读者有所耳闻。

将这对样本分别输入新旧两个模型，可以得到四个损失值，再将其带入上面的总损失公式即可。训练一段时间后，就能看到模型变得更倾向于生成五言诗。

```
<Omit many iterations here...>

Epoch 2, step 0, train loss 0.0000, evaluate loss 0.6818, train reward margin 0.0000, evaluate reward margin 0.2581
Generate a complete poem for title 春夜喜雨:
Aligned model:
翠紗復掩涴，色轉目猶輕。
素巷風無事，清池月不寒。
雲收爐尚望，日臥菊初明。
翠貼霏逾老，香藏遠獨紅。
Reference model:
黔城風光雪老遷，不將岸上林間樹。
莫道鸚鵡詩想熟，此催黃柳老風摧。
```

在本节的最后，让我们把视角稍微拉回现实世界。事实上，本节介绍的方法称为DPO（Direct Preference Optimization，直接偏好优化），它只是对齐方法的一种，甚至不是最主流的。

OpenAI最初提出的方法要更有名气一些，称为RLHF（Reinforcement Learning from Human Feedback，人类反馈的强化学习）。当时，为了将模型的价值观与人类对齐，研究者们认为是时候引入人工反馈了。但如果只是人工标注数据然后丢给模型学习，投入产出比太低，毕竟OpenAI的人没那些闲功夫天天标注数据。于是，他们开始琢磨强化学习这条路。

强化学习，简单来说就是一种终极目标可衡量的学习过程。比如下围棋，下到最后输或是赢结果非常明确，绝不存在争议。几乎所有的游戏、运动类项目都可以认为是强化学习任务，这也解释了为什么基于强化学习的AlphaGo系列最先在围棋和Dota等游戏中获得成功。为什么能衡量终极目标这一点如此重要呢？想象我们人类自己，当你有一个明确的目标时，会不会变得更有动力。其中的道理并不复杂，抵达终极目标的道路是有限的，从固定的目标倒退过程，我们就有可能找到抵达目标的最短路径。至少，最短路径理论上存在。反之，如果没有目标，我们甚至无从思考努力的方向。这个世界的可能性无限大，缺乏目标约束的人生可能会走得非常随机，许多人会在中途失去动力。

然而，说话并不是一个有明确目标的任务，所以历史上并没有人用强化学习打造语言模型。但回到偏好对齐这个任务上，它的目标似乎稍微明确了一些。假设我是一个价值观评判师，符合我价值观的文本我给它评1分，不符合我价值观的文本评-1分。OpenAI的人认为我的价值观非常符合它们对ChatGPT的期待，于是邀请我去给ChatGPT打分。他们使用一种复杂的强化学习框架（PPO），每次让ChatGPT生成一个回答，交给我打分。打分后，强化学习框架会根据这个分数微调模型的参数，目标是下次生成的回答得分更高。经过这套训练流程，模型会越来越符合我的评价，从而实现与我对齐。当然，以上是一个通俗解释。实际上，我的作用被一个奖励模型（Reward Model）取代，它可以更快地为每个回答提供打分，毕竟，真人在这个训练流程中肯定是跟不上节奏的。

RLHF从提出至今仍然占据着主流地位，几个头部商用大模型（ChatGPT，Claude，Gemini）都采用了这种方法。而DPO来自于斯坦福大学的研究团队，他们从理论上证明了使用正负样本对来训练本质上和强化学习是一样的。不过，时过境迁，DPO并没有代替RLHF，而只是在较小规模的模型上有所应用，在超大模型的情况下表现欠佳。有许多学者尝试改进DPO，并提出了许多理论以说明DPO相比于RLHF的弱点。这里面的门道很深，本文就此打住。
